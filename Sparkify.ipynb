{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Level Sparkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spark content\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Pre-processing packages\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, desc, col, max as scala_max, sum as scala_sum, split as scala_split\n",
    "from pyspark.sql.functions import size as scala_len\n",
    "from pyspark.sql.functions import sum as scala_sum, unix_timestamp, avg as scala_avg, count as scala_count\n",
    "from pyspark.sql.functions import from_unixtime, col\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from pyspark.sql.types import StringType, BooleanType, TimestampType, ArrayType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Machine Learning Packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "\n",
    "# Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30'),\n",
       " Row(artist='Five Iron Frenzy', auth='Logged In', firstName='Micah', gender='M', itemInSession=79, lastName='Long', length=236.09424, level='free', location='Boston-Cambridge-Newton, MA-NH', method='PUT', page='NextSong', registration=1538331630000, sessionId=8, song='Canada', status=200, ts=1538352180000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.103 Safari/537.36\"', userId='9')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['firstName', 'lastName', 'auth', 'method', 'sessionId', 'song', \n",
    "             'status', 'registration', 'userAgent', 'artist', 'itemInSession']\n",
    "\n",
    "for drop in drop_list:\n",
    "    df = df.drop(drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|154578|\n",
      "|  null|  8346|\n",
      "|     M|123576|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            location|count|\n",
      "+--------------------+-----+\n",
      "|     Gainesville, FL| 1229|\n",
      "|Atlantic City-Ham...| 2176|\n",
      "|Deltona-Daytona B...|   73|\n",
      "|San Diego-Carlsba...|  754|\n",
      "|Cleveland-Elyria, OH| 1392|\n",
      "|Kingsport-Bristol...| 1863|\n",
      "|New Haven-Milford...| 4007|\n",
      "|Birmingham-Hoover...|   75|\n",
      "|  Corpus Christi, TX|   11|\n",
      "|         Dubuque, IA|  651|\n",
      "|Las Vegas-Henders...| 2042|\n",
      "|Indianapolis-Carm...|  970|\n",
      "|Seattle-Tacoma-Be...|  246|\n",
      "|          Albany, OR|   23|\n",
      "|   Winston-Salem, NC|  819|\n",
      "|     Bakersfield, CA| 1775|\n",
      "|Los Angeles-Long ...|30131|\n",
      "|Minneapolis-St. P...| 2134|\n",
      "|San Francisco-Oak...| 2647|\n",
      "|Phoenix-Mesa-Scot...| 4846|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stateCode Feature Engineering (dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stateCode=' IL-IN-WI', location='Chicago-Naperville-Elgin, IL-IN-WI')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grp = df.withColumn(\"stateCode\", scala_split(df.location, ',')\\\n",
    "    .getItem(1))\\\n",
    "    .select([\"stateCode\", 'location'])\\\n",
    "    .distinct()\n",
    "\n",
    "df_grp.filter(df_grp.stateCode == ' IL-IN-WI').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It contains multiple locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiState Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|multiState|\n",
      "+----------+\n",
      "|         1|\n",
      "|         0|\n",
      "+----------+\n",
      "\n",
      "+------+---------+-----+---------------+-------------+------+----------+\n",
      "|gender|   length|level|           page|           ts|userId|multiState|\n",
      "+------+---------+-----+---------------+-------------+------+----------+\n",
      "|     M|277.89016| paid|       NextSong|1538352117000|    30|         0|\n",
      "|     M|236.09424| free|       NextSong|1538352180000|     9|         1|\n",
      "|     M| 282.8273| paid|       NextSong|1538352394000|    30|         0|\n",
      "|     M|262.71302| free|       NextSong|1538352416000|     9|         1|\n",
      "|     M|223.60771| paid|       NextSong|1538352676000|    30|         0|\n",
      "|     M|208.29995| free|       NextSong|1538352678000|     9|         1|\n",
      "|     M|260.46649| free|       NextSong|1538352886000|     9|         1|\n",
      "|     M|185.44281| paid|       NextSong|1538352899000|    30|         0|\n",
      "|     M|     null| paid|Add to Playlist|1538352905000|    30|         0|\n",
      "|     M|134.47791| paid|       NextSong|1538353084000|    30|         0|\n",
      "|     M| 229.8771| free|       NextSong|1538353146000|     9|         1|\n",
      "|     M|     null| free|    Roll Advert|1538353150000|     9|         1|\n",
      "|     M|223.58159| paid|       NextSong|1538353218000|    30|         0|\n",
      "|     M|201.06404| free|       NextSong|1538353375000|     9|         1|\n",
      "|     M|     null| free|      Thumbs Up|1538353376000|     9|         1|\n",
      "|     M|246.69995| paid|       NextSong|1538353441000|    30|         0|\n",
      "|     M|168.64608| free|       NextSong|1538353576000|     9|         1|\n",
      "|     F| 166.1122| free|       NextSong|1538353668000|    74|         0|\n",
      "|     M|222.22322| paid|       NextSong|1538353687000|    30|         0|\n",
      "|     M|229.77261| free|       NextSong|1538353744000|     9|         1|\n",
      "+------+---------+-----+---------------+-------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_state_indicator = udf(lambda x: 1 if x > 1 else 0, IntegerType())\n",
    "\n",
    "df = df.withColumn(\"stateCode\", scala_split(df.location, ',').getItem(1)).drop('location')\n",
    "df = df.withColumn(\"multiState\", multi_state_indicator(scala_len(scala_split(df.stateCode, '-')))).drop('stateCode')\n",
    "df.select('multiState').distinct().show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"page\").distinct().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# date addition\n",
    "to_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).strftime(\"%Y-%m-%d\"))\n",
    "df = df.withColumn(\"date\", to_datetime(df.ts))\n",
    "\n",
    "# datime addition\n",
    "to_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).isoformat())\n",
    "df = df.withColumn(\"datetime\", to_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Cancellation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window function for values unbounded preceding the label\n",
    "window_fun = (Window.partitionBy(\"userId\")\n",
    "             .orderBy(desc(\"ts\"))\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "# flagging cancellations\n",
    "def user_defined_function(phase): \n",
    "    return udf(lambda x: \n",
    "               1 if x == phase\n",
    "               else 0, IntegerType())\n",
    "    \n",
    "flag_cancel_func = user_defined_function(\"Cancellation Confirmation\")\n",
    "\n",
    "# adding downgrade indicator indicator\n",
    "df = df.withColumn(\"cancel_ind\", flag_cancel_func(\"page\"))\n",
    "\n",
    "# adding churn flag\n",
    "df = df.withColumn(\"cancel_user\", \n",
    "                   scala_sum(\"cancel_ind\").over(window_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downgrade Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagging downgrades\n",
    "flag_downgraded_func = user_defined_function(\"Submit Downgrade\")\n",
    "\n",
    "# adding indicator\n",
    "df = df.withColumn(\"downgrade_ind\", flag_downgraded_func(\"page\"))\n",
    "\n",
    "# sum flags over the window statement\n",
    "df = df.withColumn(\"downgrade_phase\", \n",
    "                   scala_sum(\"downgrade_ind\").over(window_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Period Between Downgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Spark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "period_function = udf(lambda page: int(page=='Downgrade'), IntegerType())\n",
    "\n",
    "over_ts_func = (Window.partitionBy(\"userId\")\n",
    "    .orderBy(desc(\"ts\"))\n",
    "    .rangeBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "df_ready = (df.filter((df.page == 'NextSong') | (df.page == 'Downgrade'))\n",
    "    .select([\"userId\", \"page\", \"gender\", \"length\", \"ts\", \"multiState\", \"level\", \"date\", \"cancel_user\"])\n",
    "    .withColumn(\"downgrade_ind\", period_function(col(\"page\")))\n",
    "    .withColumn(\"period\", scala_sum(\"downgrade_ind\").over(over_ts_func))\n",
    ").filter(df.userId == '200011')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period SQL Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"sparkify_table\")\n",
    "\n",
    "# downgrade_indicator = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     userId, \n",
    "#     page, \n",
    "#     gender,\n",
    "#     level,\n",
    "#     length,\n",
    "#     date,\n",
    "#     ts,\n",
    "#     multiState,\n",
    "#     CASE \n",
    "#         WHEN page = 'Downgrade' \n",
    "#         THEN 1 \n",
    "#         ELSE 0 \n",
    "#     END AS downgrade_ind,\n",
    "#     cancel_user\n",
    "# FROM sparkify_table\n",
    "# WHERE (page = 'NextSong') or (page = 'Downgrade')\n",
    "# \"\"\")\n",
    "\n",
    "# downgrade_indicator.createOrReplaceTempView('downgrade_table')\n",
    "\n",
    "# df_ready = spark.sql(\"\"\"\n",
    "# SELECT\n",
    "#     *,\n",
    "#     SUM(downgrade_ind)\n",
    "#     OVER(PARTITION BY userId \n",
    "#         ORDER BY ts DESC \n",
    "#         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS downgrade_period\n",
    "# FROM downgrade_table\n",
    "# --WHERE userId = 200011\n",
    "# \"\"\")\n",
    "\n",
    "# df_ready = df_ready.drop('downgrade_ind')\n",
    "\n",
    "# df_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Endoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('gender').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*null genders will have 0 and 0 for both Male and Female columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_encoder = udf(lambda x: 1 if x == 'M' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn(\"Male\", male_encoder(df_ready.gender))\n",
    "\n",
    "female_encoder = udf(lambda x: 1 if x == 'F' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn(\"Female\", female_encoder(df_ready.gender))\n",
    "\n",
    "df_ready = df_ready.drop('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('level').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_encoder = udf(lambda x: 1 if x == 'free' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn('free', free_encoder(df_ready.level))\n",
    "\n",
    "paid_encoder = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn('paid', paid_encoder(df_ready.level))\n",
    "\n",
    "df_ready = df_ready.drop('level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Seconds Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_date_func = (Window.partitionBy([\"userId\", \"date\"]))\n",
    "\n",
    "# also filtering only on songs\n",
    "df_ready = df_ready.filter((df.page == 'NextSong'))\\\n",
    "    .withColumn(\"DailySeconds\", scala_sum(\"length\").over(over_date_func))\n",
    "\n",
    "df_ready = df_ready.drop('length').drop('page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Use Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_ready.withColumn(\"DailyUse\", scala_count(\"userId\").over(over_date_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get available columns\n",
    "feature_columns = df_ready.columns\n",
    "\n",
    "# removed column\n",
    "for col in ['userId', 'date', 'cancel_user', 'ts']:\n",
    "    feature_columns.remove(col)\n",
    "\n",
    "print(\"Feature Columns\")\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_ready\n",
    "\n",
    "# assembling the predictor features\n",
    "feature_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "data = feature_assembler.transform(data)\n",
    "\n",
    "# indexing the labels\n",
    "labelIndexer = StringIndexer(inputCol=\"cancel_user\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "# indexit the features\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# index to string predictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = data.randomSplit([0.7, 0.3], seed=42)\n",
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Validation Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name in ['accuracy', 'weightedPrecision', 'weightedRecall', 'f1']:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=metric_name)\n",
    "    accuracy = evaluator.evaluate(pred)\n",
    "    print(f\"{metric_name} = {accuracy:2.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30])\\\n",
    "    .addGrid(rf.maxDepth, [4, 8, 12])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, \n",
    "                          estimatorParamMaps=paramGrid, \n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol='indexedLabel', \n",
    "                                                                      predictionCol='prediction', \n",
    "                                                                      metricName='accuracy'), \n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmodel = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = cvmodel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(cv_pred.rdd.map(lambda x: (x.prediction, x.indexedLabel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "accuracy = metrics.accuracy\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Accuracy = %s\" % accuracy)\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics:**\n",
    "\n",
    "Accuracy: $\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Precission: $\\frac{TP}{TP+FP}$\n",
    "\n",
    "Recall: $\\frac{TP}{TP+FN}$\n",
    "\n",
    "f1: $2 *\\frac{precision * recall}{precision + recall}$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "475px",
    "width": "360.199px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
