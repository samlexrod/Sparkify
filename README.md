# Sparkify:
I chose this project since it would give me the experience I need to apply big data techniques in my current job. We currently use AWS as our cloud infracstructure so I saw the benefits of it. The experience gained were far beyong what I was expecting.

# Packages used:
    pyspark
    datetime
    pandas
    
This is the import statement:
```python
# find spark content in local mode only
import findspark
findspark.init()

# Pre-processing packages
import datetime
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, desc, col, max as scala_max, sum as scala_sum, split as scala_split
from pyspark.sql.functions import size as scala_len
from pyspark.sql.functions import sum as scala_sum, unix_timestamp, avg as scala_avg, count as scala_count
from pyspark.sql.functions import from_unixtime, col
from pyspark.sql.types import DateType, IntegerType
from pyspark.sql.types import StringType, BooleanType, TimestampType, ArrayType
from pyspark.sql import Window

# Machine Learning Packages
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator

# Random Forest
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics
```

# Installation of Spark in Localhost mode:

1. Download spark 2.3.3 pre-built for Apache Hadoop 2.7 and later
https://spark.apache.org/downloads.html
- Extract spark tgz file
- Create path C:\spark and copy the whole extracted folder
- Go to C:\spark\conf\log4j.properties.template and delete .templete leave just log4j.properties
- Open the file and change INFO to ERROR on log4j.rootCategory

2. Download Java SE Development Kit 8u211
https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
- Install Java SDK in C:\jdk

3. Download winutils.exe
https://github.com/steveloughran/winutils/blob/master/hadoop-2.6.0/bin/winutils.exe
- Create path in C:\winutils\bin and paste the executable in it

4. In start menu type Environment Variables and enter the Environment Variables
- Create new user variable called SPARK_HOME and set the variable value to be 
    C:\spark
- Create another new user variable called JAVA_HOME and set the variable value to be
    C:\jdk
- Create another new user variable called HADOOP_HOME and set the variable value to be
    C:\winutils
    
5. Go to PATH and double click and enter the new environment variables
- %SPARK_HOME%\bin
- %JAVA_HOME%\bin

6. Install pyspark and findspark by typing the following in command prompt
- pip install pyspark
- pip isntall findspark

Note: make sure to start your imports as 
```python
import findspark
findspark.init()
```

# Files in repository
- Anything inside the ArticleImg are files related to the article.
- metastore_db and spark-warehouse can be ignored as is automatically generated by the spark session.
- Sparkify.ipynb contains the script of the prototype final model to be approved by the business owner.
- LICENSE has the license, hence the name.
- You are reading the README.md

# Summary of Results
The accuracy is the overall accuracy of the model.
The precision focuses on how many users we expect to churn that actually churn taking into consideration incorrect users expected to churn.
The recall focuses on how many users we expected to churn that actually churn taking into consideration incorrect users expected to stay.
The f1 score takes into consideration both precision and recall.

The model evaluations without applying the cross-validation are as follows:
- Accuracy: 84.20%
- wightedPrecision: 86.71%
- weightedRecall: 84.20%
- f(1): 77.30%

The model evaluations after applying the cross-validation are as follows:
- Accuracy: 87.20%
- wightedPrecision: 88.46%
- weightedRecall: 87.20%
- f(1): 83.56%

The model applied here is a well-balanced model that takes into consideration false positives and false negatives. So we are ready to start providing recommendations to the respective team in charge of user retention.

# Reflection
The problem here is that we want to know when users are about to leave the services. We want to retain these users because lossing them means lossing revenue due to the advertising channel or the paid service channel.hence, the marketing department will be interested in this results. It was interesting to think about this issue in the perspective of a cloud engineer, data engineer, and data scientist. Thinking on the limitations that my current infracstucutre will face and solving the big data issue using the AWS EMR clusters is an important aspect of a data scientist job.  
