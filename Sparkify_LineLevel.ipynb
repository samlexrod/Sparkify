{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Level Sparkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find spark content\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Pre-processing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, desc, col, max as scala_max, sum as scala_sum\n",
    "from pyspark.sql.functions import sum as scala_sum, unix_timestamp, avg as scala_avg, count as scala_count\n",
    "from pyspark.sql.functions import from_unixtime, col\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from pyspark.sql.types import StringType, BooleanType, TimestampType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Machine Learning Packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "\n",
    "# # Logistic Regression\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# # Gradient-bossted Tree Regression\n",
    "# from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# date addition\n",
    "to_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).strftime(\"%Y-%m-%d\"))\n",
    "df = df.withColumn(\"date\", to_datetime(df.ts))\n",
    "\n",
    "# datime addition\n",
    "to_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).isoformat())\n",
    "df = df.withColumn(\"datetime\", to_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Cancellation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window function for values unbounded preceding the label\n",
    "window_fun = (Window.partitionBy(\"userId\")\n",
    "             .orderBy(desc(\"ts\"))\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "# flagging cancellations\n",
    "def user_defined_function(phase): \n",
    "    return udf(lambda x: \n",
    "               1 if x == phase\n",
    "               else 0, IntegerType())\n",
    "    \n",
    "flag_cancel_func = user_defined_function(\"Cancellation Confirmation\")\n",
    "\n",
    "# adding downgrade indicator indicator\n",
    "df = df.withColumn(\"cancel_ind\", flag_cancel_func(\"page\"))\n",
    "\n",
    "# adding churn flag\n",
    "df = df.withColumn(\"cancel_user\", \n",
    "                   scala_sum(\"cancel_ind\").over(window_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downgrade Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagging downgrades\n",
    "flag_downgraded_func = user_defined_function(\"Submit Downgrade\")\n",
    "\n",
    "# adding indicator\n",
    "df = df.withColumn(\"downgrade_ind\", flag_downgraded_func(\"page\"))\n",
    "\n",
    "# sum flags over the window statement\n",
    "df = df.withColumn(\"downgrade_phase\", \n",
    "                   scala_sum(\"downgrade_ind\").over(window_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Period Between Downgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Spark Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "period_function = udf(lambda page: int(page=='Downgrade'), IntegerType())\n",
    "\n",
    "over_ts_func = (Window.partitionBy(\"userId\")\n",
    "    .orderBy(desc(\"ts\"))\n",
    "    .rangeBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "df_ready = (df.filter((df.page == 'NextSong') | (df.page == 'Downgrade'))\n",
    "    .select([\"userId\", \"page\", \"gender\", \"length\", \"ts\"])\n",
    "    .withColumn(\"downgrade_ind\", period_function(col(\"page\")))\n",
    "    .withColumn(\"period\", scala_sum(\"downgrade_ind\").over(over_ts_func))\n",
    ").filter(df.userId == '200011')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period SQL Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId='100010', page='NextSong', gender='F', level='free', length=185.25995, date='2018-11-21', ts=1542823951000, cancel_user=0, downgrade_period=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sparkify_table\")\n",
    "\n",
    "downgrade_indicator = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    userId, \n",
    "    page, \n",
    "    gender,\n",
    "    level,\n",
    "    length,\n",
    "    date,\n",
    "    ts,\n",
    "    CASE \n",
    "        WHEN page = 'Downgrade' \n",
    "        THEN 1 \n",
    "        ELSE 0 \n",
    "    END AS downgrade_ind,\n",
    "    cancel_user\n",
    "FROM sparkify_table\n",
    "WHERE (page = 'NextSong') or (page = 'Downgrade')\n",
    "\"\"\")\n",
    "\n",
    "downgrade_indicator.createOrReplaceTempView('downgrade_table')\n",
    "\n",
    "df_ready = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    SUM(downgrade_ind)\n",
    "    OVER(PARTITION BY userId \n",
    "        ORDER BY ts DESC \n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS downgrade_period\n",
    "FROM downgrade_table\n",
    "--WHERE userId = 200011\n",
    "\"\"\")\n",
    "\n",
    "df_ready = df_ready.drop('downgrade_ind')\n",
    "\n",
    "df_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Endoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|  null|\n",
      "|     M|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('gender').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*null genders will have 0 and 0 for both Male and Female columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_encoder = udf(lambda x: 1 if x == 'M' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn(\"Male\", male_encoder(df_ready.gender))\n",
    "\n",
    "female_encoder = udf(lambda x: 1 if x == 'F' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn(\"Female\", female_encoder(df_ready.gender))\n",
    "\n",
    "df_ready = df_ready.drop('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('level').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_encoder = udf(lambda x: 1 if x == 'free' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn('free', free_encoder(df_ready.level))\n",
    "\n",
    "paid_encoder = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "df_ready = df_ready.withColumn('paid', paid_encoder(df_ready.level))\n",
    "\n",
    "df_ready = df_ready.drop('level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Seconds Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- cancel_user: long (nullable = true)\n",
      " |-- downgrade_period: long (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Female: integer (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ready.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_date_func = (Window.partitionBy([\"userId\", \"date\"]))\n",
    "\n",
    "# also filtering only on songs\n",
    "df_ready = df_ready.filter((df.page == 'NextSong'))\\\n",
    "    .withColumn(\"DailySeconds\", scala_sum(\"length\").over(over_date_func))\n",
    "\n",
    "df_ready = df_ready.drop('length').drop('page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Use Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_ready.withColumn(\"DailyUse\", scala_count(\"userId\").over(over_date_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['downgrade_period',\n",
       " 'Male',\n",
       " 'Female',\n",
       " 'free',\n",
       " 'paid',\n",
       " 'DailySeconds',\n",
       " 'DailyUse']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get available columns\n",
    "feature_columns = df_ready.columns\n",
    "\n",
    "# removed column\n",
    "for col in ['userId', 'date', 'cancel_user', 'ts']:\n",
    "    feature_columns.remove(col)\n",
    "\n",
    "print(\"Feature Columns\")\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- cancel_user: long (nullable = true)\n",
      " |-- downgrade_period: long (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Female: integer (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- DailySeconds: double (nullable = true)\n",
      " |-- DailyUse: long (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df_ready\n",
    "\n",
    "# modeling the data\n",
    "feature_assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "data = feature_assembler.transform(data)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"cancel_user\", outputCol=\"indexedLabel\").fit(data)\n",
    "labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {0.0: 0, 1.0: 1}, 2: {0.0: 0, 1.0: 1}, 3: {0.0: 0, 1.0: 1}, 4: {0.0: 0, 1.0: 1}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "featureIndexer.categoryMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- cancel_user: long (nullable = true)\n",
      " |-- downgrade_period: long (nullable = true)\n",
      " |-- Male: integer (nullable = true)\n",
      " |-- Female: integer (nullable = true)\n",
      " |-- free: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- DailySeconds: double (nullable = true)\n",
      " |-- DailyUse: long (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modeling predictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160001, 68107)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData, testData = data.randomSplit([0.7, 0.3], seed=42)\n",
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Validation Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 84.08%\n",
      "weightedPrecision = 86.62%\n",
      "weightedRecall = 84.08%\n",
      "f1 = 77.00%\n"
     ]
    }
   ],
   "source": [
    "for metric_name in ['accuracy', 'weightedPrecision', 'weightedRecall', 'f1']:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=metric_name)\n",
    "    accuracy = evaluator.evaluate(pred)\n",
    "    print(f\"{metric_name} = {accuracy:2.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30])\\\n",
    "    .addGrid(rf.maxDepth, [4, 8, 12])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, \n",
    "                          estimatorParamMaps=paramGrid, \n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol='indexedLabel', \n",
    "                                                                      predictionCol='prediction', \n",
    "                                                                      metricName='accuracy'), \n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmodel = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = cvmodel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred.select(\"predictedLabel\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(cv_pred.rdd.map(lambda x: (x.prediction, x.indexedLabel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "accuracy = metrics.accuracy\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Accuracy = %s\" % accuracy)\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "475px",
    "width": "360.199px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
